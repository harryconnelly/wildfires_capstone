# wildfires_capstone


### Capstone Wildfires



##### WHY: Wildfires in California made me particularly interested in this piece of data. The incredibly damanging, dangerous and publicly recorded wildfires in California stuck with me from last year. I was also swapping stories with a friend and heard about a 15 yearold boy who accidentally started a massive wildfire in Oregon. The damage was incredible, and what if we could determine when conditions are most dangerous? There are many different factors including time of year, weather, environnment, and human actions that lead to factors of likelihood of wildfires. 

##### GOAL: What if we could see conditions that are optimal for large, dangerous wildfire situations so that we could warn people and educate them on behavior. Close certain parks during certain times of year? Ultimately I divided the United States into a grid through categories based on latitude and longitude. I then compared all these observations. 

##### DATA LOCATION: I had looked at some other data sets that were of interest, but I felt that having a solid y variable and getting creative leveraging predictions could be an optimal way to leverage these results. This is a link to the wildfire data: https://www.kaggle.com/rtatman/188-million-us-wildfires. It is in SQL and too large to be uploaded onto github. With this data, I supplimented it with other data around it. This includes day of the week data. 

##### DATA MANIPULATION: The data was given by location (latitude and longitude) and the date which were my primary focuses, along with other important details like the cause, state, county. As stated earlier, I condensed these 1.88 million datapoints into quandrants rounded to the nearest 2. So if a latitude point was 53.1 it was rounded to 54, and if it was 52.9 it was rounded to 52. This made the data far more condensed and interpretable. Other data used, I created my own excel file that had dates and day of the week. Keep in mind this data is in Julian Calendar, so it needed to be converted. Because the data was in Julian Calendar, I was actually able to do some interesting things, but I needed it accurately in the way most people interpret calendars. I saw that excel often auto fills, and stops at the 30th, but it is not truly exact. I wanted and needed the dates to be exactly right, so I manually made myself my own excel sheet that is exactly correct on dates and was able to accurtely incorporate it within my model.  

##### DATA VALUES: Key pieces of data I looked at were date, wildfire size category, wildfire size and location. The date was listed in Julian. I was able to convert it over to the traditional but kept the Julian calendar as well. Wildfire size was by Acres. There are many different sizes and categories, but A is the smallest, and G is the largest at over 5,000 acres. Wildfire size is taken in acres. 

##### TARGET VARIABLE: The target variable is the differential. I am looking for times there are major jumps in size of Forrest Fires. I originally wanted to predict forest fires over a certain size but it became apparent that forestfires are highly correlated with eachother. So if there are forrest fires one day, there are likely forrest fires the next. A bit like temperature, and the data was too correlated for it to be interesting. I am more interested for in when to start being worried, than guessing whether a day will have them or not. So I ended up looking at the difference betweeen wildfire size in a location over time. If the difference in a grid location is 4x or +20,000 acres after the past numbers was zero it is documented. 

 
##### MODELING TECHNIQUES: My statistical analysis at this leveraged a neural network using a softmax which are ideal for classification. Organizing the data in a way I wanted took an extremely long time. I am trying to build different features and keep the data clean and standardized. The functions I chose to use are more efficient than for looping through everything and for looping through itself which whas I originally thought of. I instead ordered everything in such away that appending would be optimal. These functions would take multiple days due to the fact that the data set is so large. This is where the checkerboard portion mentioned above comes into place. With everything being grouped into longitude and latitude I summed up the vairous amounts and created max values. I also looked at the different dispersion of category fires summed up. So if there were 4 wildfires that were category A, then the category A column got a 4. I then lagged that information and summed them up. With this lagging information, I was able to compare what an area was like 3 months prior, a year prior and for years prior. This data is good information for indicating what will happen the next year. I was seeing a trend that the amount of wildfires within 15 days was too indicative of whether there was a large wild fire or not. The reality is that wildfires spread, and so I was essentially making a far too simple prediction, and it might not even have beena prediction. Thus I took out the sizes of the fires from days 10 in, and instead looked at the difference in wildfires acre sum and the day we were looking at. I then used that as the TARGET. If there was a major jump in action that is what I was looking for. In any scenario where the fires jumped 4x or 2x at a certain size or jumped up to a certain amount I designated it a 1, and everything else a 0. This is what I am testing for, not if a fire is over a given size.  

###### FINAL GOAL: Another critical thing that is critical is that what I am trying to look for is if an area is likely to have a change in wildfire size so that the land is covered greatly. The idea is that once we are able to determine the area of highest risk, we can build a model for that given location and evaluate where it is most likely to occur after that. By predicting and figuring out the areas of highest risk, we can police and warn people optimally. This would save lives and hopefully reduce the amount of people in danger and accidental wildfires started. 


##### IMPORTANT DISCLOSURES: Data is too large for to be viewed in one github file, so I chopped it up to make it so that people could read it/ see the work, rather than downloading all of it. Data is also too large to be uploaded to github, it is past the requirements, so if you would like to follow along, please download the SQL file that I linked above. 
