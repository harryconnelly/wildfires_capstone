{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1000=pd.read_csv('datacleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1000.drop(columns='diffmax', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re prep data\n",
    "\n",
    "##### clear out all data below 2003. Looking at it it was far more sparingly collected and less consistent. I did not feel it was logical to assume that there are just so much more wildfires occuring today, but rather the ability to record or something has dramatically changed, thus I decided to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert df1000 to datetime\n",
    "df1000['day']=pd.to_datetime(df1000['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change dataframe to be above a certain date. it became apparent that data before 2003 was far less standardized than the\n",
    "#data after\n",
    "df500 = df1000[df1000['dates']>2452640.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df500.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df500.drop(columns=['Unnamed: 0', 'index', 'level_0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#convert data as type int\n",
    "df500['dates']=df500['dates'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do range with floats or figure out why they julian days are repeating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Index and Fill in Missing Columns\n",
    "#### Need to look at historical data and put it in rows. In order to do it efficiently and accurately I need to inclue all dates even the ones that are blanks, and I need to have the data ordered by 'lat lon' and then by date. through that I can use if then statements to filter data how I want which you will see further down\n",
    "\n",
    "\n",
    "#### Below is me working with resetting the index and getting all the data points in properly included. So even though I did not have an observation on a given day, it is still in the dataframe and in order and accounted for. Julian Calendars made this far easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this was a doosy\n",
    "#first looping through value counts on a reset index so that I can pull out and loop through all the latitude and longitudes \n",
    "#and automatically plug them in\n",
    "\n",
    "#second loop I reindexed the data properly on the dates (julian calendar)\n",
    "\n",
    "for i,y in enumerate(df500['lat lon'].value_counts().reset_index()['index']):\n",
    "    idx = range(2452641, 2457387, 1)\n",
    "    if i == 0:\n",
    "        df5000 = df500.set_index('dates')\n",
    "        s = 0\n",
    "        s = df5000[df5000['lat lon']==y]\n",
    "        z = 0\n",
    "        z = s.reindex(idx, fill_value = 0)\n",
    "    else:\n",
    "        df2500 = df500.set_index('dates')\n",
    "        s = 0\n",
    "        s = df2500[df2500['lat lon']== y]\n",
    "        q = 0\n",
    "        q = s.reindex(idx, fill_value = 0)\n",
    "        z=pd.concat([z,q], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after all that beautiful work is done, definitely want to reset the index\n",
    "z.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z['A_']=z['A'].shift(-1)\n",
    "z['B_']=z['B'].shift(-1)\n",
    "z['C_']=z['C'].shift(-1)\n",
    "z['D_']=z['D'].shift(-1)\n",
    "z['E_']=z['E'].shift(-1)\n",
    "z['F_']=z['F'].shift(-1)\n",
    "z['G_']=z['G'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z['FIRE SIZE SUM_']=z['FIRE SIZE SUM'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['FIRE SIZE MAX_']=z['FIRE SIZE MAX'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dates', 'latit', 'longit', 'longitude', 'latitude', 'state',\n",
       "       'sz_class', 'cause', 'firesize', 'A', 'B', 'C', 'D', 'E', 'F', 'G',\n",
       "       'FIRE SIZE MAX', 'FIRE SIZE SUM', 'fire binary', 'weekend', 'day',\n",
       "       'day of week', 'lat lon', 'A_', 'B_', 'C_', 'D_', 'E_', 'F_', 'G_',\n",
       "       'FIRE SIZE SUM_', 'FIRE SIZE MAX_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z['A_45']=z['A'].rolling(window=30).sum().shift(-60)\n",
    "z['B_45']=z['B'].rolling(window=30).sum().shift(-60)\n",
    "z['C_45']=z['C'].rolling(window=30).sum().shift(-60)\n",
    "z['D_45']=z['D'].rolling(window=30).sum().shift(-60)\n",
    "z['E_45']=z['E'].rolling(window=30).sum().shift(-60)\n",
    "z['F_45']=z['F'].rolling(window=30).sum().shift(-60)\n",
    "z['G_45']=z['G'].rolling(window=30).sum().shift(-60)\n",
    "z['FIRE SIZE SUM_45']=z['FIRE SIZE SUM'].rolling(window=30).sum().shift(-60)\n",
    "z['FIRE SIZE MAX_45']=z['FIRE SIZE MAX'].rolling(window=30).sum().shift(-60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['A_365']=z['A'].rolling(window=90).sum().shift(-400)\n",
    "z['B_365']=z['B'].rolling(window=90).sum().shift(-400)\n",
    "z['C_365']=z['C'].rolling(window=90).sum().shift(-400)\n",
    "z['D_365']=z['D'].rolling(window=90).sum().shift(-400)\n",
    "z['E_365']=z['E'].rolling(window=90).sum().shift(-400)\n",
    "z['F_365']=z['F'].rolling(window=90).sum().shift(-400)\n",
    "z['G_365']=z['G'].rolling(window=90).sum().shift(-400)\n",
    "z['FIRE SIZE SUM_365']=z['FIRE SIZE SUM'].rolling(window=90).sum().shift(-400)\n",
    "z['FIRE SIZE MAX_365']=z['FIRE SIZE MAX'].rolling(window=90).sum().shift(-400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['A_2yr']=z['A'].rolling(window=365).sum().shift(-912)\n",
    "z['B_2yr']=z['B'].rolling(window=365).sum().shift(-912)\n",
    "z['C_2yr']=z['C'].rolling(window=365).sum().shift(-912)\n",
    "z['D_2yr']=z['D'].rolling(window=365).sum().shift(-912)\n",
    "z['E_2yr']=z['E'].rolling(window=365).sum().shift(-912)\n",
    "z['F_2yr']=z['F'].rolling(window=365).sum().shift(-912)\n",
    "z['G_2yr']=z['G'].rolling(window=365).sum().shift(-912)\n",
    "z['FIRE SIZE SUM_2yr']=z['FIRE SIZE SUM'].rolling(window=90).sum().shift(-912)\n",
    "z['FIRE SIZE MAX_2yr']=z['FIRE SIZE MAX'].rolling(window=90).sum().shift(-912)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['A_7']=z['A'].rolling(window=7).sum().shift(-7)\n",
    "z['B_7']=z['B'].rolling(window=7).sum().shift(-7)\n",
    "z['C_7']=z['C'].rolling(window=7).sum().shift(-7)\n",
    "z['D_7']=z['D'].rolling(window=7).sum().shift(-7)\n",
    "z['E_7']=z['E'].rolling(window=7).sum().shift(-7)\n",
    "z['F_7']=z['F'].rolling(window=7).sum().shift(-7)\n",
    "z['G_7']=z['G'].rolling(window=7).sum().shift(-7)\n",
    "z['FIRE SIZE SUM_7']=z['FIRE SIZE SUM'].rolling(window=7).sum().shift(-7)\n",
    "z['FIRE SIZE MAX_7']=z['FIRE SIZE MAX'].rolling(window=7).sum().shift(-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of Max Fire Acreage and Sum of total fire Acreage\n",
    "\n",
    "#### did this over various intervals. turns out the time interval can be too close and not really helpful when you are within 10 days.\n",
    "#### did incraments of 4 years before, 1 year before, and 45 days before. These are obviously each a range but the data was extremely helpful. ALSO this means that I will have to cut 4 years later on because with a 4 year lag there is not data on those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using .diff to see difference between current value and values prior. This is SUPER IMPORTANT. These will be my y variables. I am not interested in finding when will there be a lot of forest fire but rather when will there be a change in conditions that will cause a change and expansion of forest fires.\n",
    "\n",
    "##### I then took only the max value of these columns so that it is standardized nicely and consistant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = pd.DataFrame(z['FIRE SIZE SUM'].diff(2))\n",
    "diff3=pd.DataFrame(z['FIRE SIZE SUM'].diff(3))\n",
    "diff5=pd.DataFrame(z['FIRE SIZE SUM'].diff(5))\n",
    "diff4=pd.DataFrame(z['FIRE SIZE SUM'].diff(4))\n",
    "diff1=pd.DataFrame(z['FIRE SIZE SUM'].diff(1))\n",
    "diff6=pd.DataFrame(z['FIRE SIZE SUM'].diff(6))\n",
    "\n",
    "diff=pd.concat([diff2, diff3], axis=1)\n",
    "diff=pd.concat([diff, diff4], axis=1)\n",
    "diff=pd.concat([diff, diff5], axis=1)\n",
    "diff=pd.concat([diff, diff1], axis=1)\n",
    "diff=pd.concat([diff, diff6], axis=1)\n",
    "\n",
    "diffmax=diff.max(axis=1)\n",
    "diffmax=diffmax.rename(columns={0: 'diffmax'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=pd.concat([z, diffmax], axis=1)\n",
    "z.rename(columns={0: 'diffmax'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "z.to_csv('lagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the way it autofilled earlier, '1970-01-01' was the documentations way of saying na. I cut out all values of this kind\n",
    "#also removed values beforew 2007. reason for that is they do not have the lagged information and details so the variables\n",
    "#would have been all messed up.\n",
    "\n",
    "#this also accounts for me making this shift accurate as all of the details will align as the dataframe was uniform and aligned\n",
    "#by location. so the dates lined up perfectly. more efficient than a forloop\n",
    "\n",
    "##2454101== Dec 31, 2006. \n",
    "z = z[z['day']!='1970-01-01']\n",
    "dfzqq = z\n",
    "z=z[z['dates']>2453371]\n",
    "z.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert day to datetime\n",
    "z['day']=pd.to_datetime(z['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfzqq['day'] = pd.to_datetime(z['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pull out the year month and day of month so that they can be represented properly in columns\n",
    "year2 = []\n",
    "month2 = []\n",
    "dayom2 = []\n",
    "for i,y in enumerate(dfzqq['day']):\n",
    "    year2.append(str(y)[:4])\n",
    "    month2.append(str(y)[5:7])\n",
    "    dayom2.append(str(y)[8:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out the year month and day of month so that they can be represented properly in columns\n",
    "year = []\n",
    "month = []\n",
    "dayom = []\n",
    "for i,y in enumerate(z['day']):\n",
    "    year.append(str(y)[:4])\n",
    "    month.append(str(y)[5:7])\n",
    "    dayom.append(str(y)[8:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert them into dataframes, ints and name them\n",
    "year = pd.DataFrame(year).rename(columns={0: 'year'}).astype(int)\n",
    "month = pd.DataFrame(month).rename(columns={0:'month'}).astype(int)\n",
    "dayom = pd.DataFrame(dayom).rename(columns={0: 'day_of_month'}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert them into dataframes, ints and name them\n",
    "year2 = pd.DataFrame(year2).rename(columns={0: 'year'})\n",
    "month2 = pd.DataFrame(month2).rename(columns={0:'month'})\n",
    "dayom2 = pd.DataFrame(dayom2).rename(columns={0: 'day_of_month'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3500=pd.concat([dfzqq, year2], axis=1)\n",
    "df3500 = pd.concat([df3500, month2], axis=1)\n",
    "df3500 = pd.concat([df3500, dayom2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3500.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#put all together into a csv\n",
    "df3000=pd.concat([z, year], axis=1)\n",
    "df3000=pd.concat([df3000, month], axis=1)\n",
    "df3000=pd.concat([df3000, dayom], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3000.to_csv('lagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
