{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leverage logistic regression as this is classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (9,10,11,12,13,14,27,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#load in data\n",
    "df = pd.read_csv('lagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classification, traintestsplit, and counter\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dynamic Classifications Through Scoring System as to what is considred a large jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "firesizecriteria=[]\n",
    "for i,y in enumerate(df['FIRE SIZE MAX']):\n",
    "    if i%100000==0:\n",
    "        print(i)\n",
    "    #if (float(y)-500) > float(df1000['diffmax'][i]):\n",
    "   \n",
    "\n",
    "    if df['diffmax'][i] > 20000:\n",
    "            firesizecriteria.append(1)\n",
    "    elif df['diffmax'][i]*5 < df['FIRE SIZE MAX'][i]:\n",
    "        if df['diffmax'][i] > 600:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    elif df['diffmax'][i]*2 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 1000:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "\n",
    "            \n",
    "    elif df['diffmax'][i]*4 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 500:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    elif df['diffmax'][i]*100 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 200:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    \n",
    "    else:\n",
    "        firesizecriteria.append(0)\n",
    "\n",
    "#     else:\n",
    "#         print(0)   \n",
    "#         firesizecriteria.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unncessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df, pd.DataFrame(firesizecriteria).rename(columns={0:'firesize_criteria'})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.get_dummies(df['day of week'], dummy_na=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df,df5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df55=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1',\n",
       "       'index', 'Unnamed: 0.1.1.1.1', 'dates', 'latit', 'longit', 'longitude',\n",
       "       'latitude', 'state', 'sz_class', 'cause', 'firesize', 'A', 'B', 'C',\n",
       "       'D', 'E', 'F', 'G', 'FIRE SIZE SUM', 'FIRE SIZE MAX', 'diffmax',\n",
       "       'fire binary', 'weekend', 'day', 'day of week', 'lat lon', '0', '0.1',\n",
       "       '0.2', '0.3', '0.4', '0.5', 'df_30_60_sum', 'df_30_60_max',\n",
       "       'df_30_B_60_sum', 'df_30_A_60_sum', 'df_30_C_60_sum', 'df_30_D_60_sum',\n",
       "       'df_30_E_60_sum', 'df_30_F_60_sum', 'df_30_G_60_sum', 'y4sum', 'y4max',\n",
       "       'y1sum', 'y1max', 'd45sum', 'd45max', 'df_30_60_sum.1',\n",
       "       'df_30_60_max.1', 'df_30_B_60_sum.1', 'df_30_A_60_sum.1',\n",
       "       'df_30_C_60_sum.1', 'df_30_D_60_sum.1', 'df_30_E_60_sum.1',\n",
       "       'df_30_F_60_sum.1', 'df_30_G_60_sum.1', 'firesize_criteria', 'Friday',\n",
       "       'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df55.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.sort_values('dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df55=df55.reset_index().drop(columns='level_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estabolish a y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df55['firesize_criteria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.drop(columns=['lat lon', 'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1', 'index', 'Unnamed: 0.1.1.1.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.drop(columns=['day', 'firesize_criteria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.drop(columns=['day of week', 'cause', 'sz_class', 'state', 'longitude', 'latitude', 'firesize', 'df_30_A_60_sum.1', 'df_30_B_60_sum.1', 'df_30_C_60_sum.1', 'df_30_D_60_sum.1', 'df_30_E_60_sum.1', 'df_30_F_60_sum.1', 'df_30_G_60_sum.1', 'df_30_60_sum.1', 'df_30_60_max.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create X and Y variables split on Date. \n",
    "\n",
    "### This is to train based on dates. Test is assumed to be in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df55[:int(df55.shape[0]*.8)]\n",
    "X_test=df55[int(df55.shape[0]*.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y[:int(df55.shape[0]*.8)]\n",
    "y_test=y[int(df55.shape[0]*.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112136,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140171, 43)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df55.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make classifications of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_classification(n_classes=2, class_sep=2, weights=[.7,.3], n_samples=1700 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = make_classification(n_classes=2, class_sep=2, weights=[.7,.3], n_samples=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Keras to Use Neural Network that Classifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomUnderSampler(random_state=None, ratio=0.4, replacement=False,\n",
       "          return_indices=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # doctest: +NORMALIZE_WHITESPACE\n",
    "rus = RandomUnderSampler(ratio = .4)\n",
    "rus.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomUnderSampler(random_state=None, ratio=0.4, replacement=False,\n",
       "          return_indices=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # doctest: +NORMALIZE_WHITESPACE\n",
    "rus = RandomUnderSampler(ratio = .4)\n",
    "rus.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###put in input, outputs and hidden sizes\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "n_output = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add key features\n",
    "model.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model.add(Dense(n_output, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1700 samples, validate on 300 samples\n",
      "Epoch 1/17\n",
      "1700/1700 [==============================] - 0s 258us/step - loss: 0.5956 - acc: 0.7229 - val_loss: 0.4779 - val_acc: 0.7467\n",
      "Epoch 2/17\n",
      "1700/1700 [==============================] - 0s 60us/step - loss: 0.3108 - acc: 0.8747 - val_loss: 0.3984 - val_acc: 0.8233\n",
      "Epoch 3/17\n",
      "1700/1700 [==============================] - 0s 54us/step - loss: 0.2125 - acc: 0.9259 - val_loss: 0.3557 - val_acc: 0.8500\n",
      "Epoch 4/17\n",
      "1700/1700 [==============================] - 0s 59us/step - loss: 0.1677 - acc: 0.9494 - val_loss: 0.3195 - val_acc: 0.8700\n",
      "Epoch 5/17\n",
      "1700/1700 [==============================] - 0s 53us/step - loss: 0.1434 - acc: 0.9559 - val_loss: 0.3044 - val_acc: 0.8600\n",
      "Epoch 6/17\n",
      "1700/1700 [==============================] - 0s 51us/step - loss: 0.1287 - acc: 0.9588 - val_loss: 0.2898 - val_acc: 0.8767\n",
      "Epoch 7/17\n",
      "1700/1700 [==============================] - 0s 72us/step - loss: 0.1197 - acc: 0.9594 - val_loss: 0.2805 - val_acc: 0.8767\n",
      "Epoch 8/17\n",
      "1700/1700 [==============================] - 0s 65us/step - loss: 0.1133 - acc: 0.9618 - val_loss: 0.2824 - val_acc: 0.8767\n",
      "Epoch 9/17\n",
      "1700/1700 [==============================] - 0s 68us/step - loss: 0.1087 - acc: 0.9618 - val_loss: 0.2770 - val_acc: 0.8767\n",
      "Epoch 10/17\n",
      "1700/1700 [==============================] - 0s 58us/step - loss: 0.1052 - acc: 0.9629 - val_loss: 0.2769 - val_acc: 0.8700\n",
      "Epoch 11/17\n",
      "1700/1700 [==============================] - 0s 57us/step - loss: 0.1025 - acc: 0.9641 - val_loss: 0.2795 - val_acc: 0.8700\n",
      "Epoch 12/17\n",
      "1700/1700 [==============================] - 0s 60us/step - loss: 0.1002 - acc: 0.9665 - val_loss: 0.2849 - val_acc: 0.8600\n",
      "Epoch 13/17\n",
      "1700/1700 [==============================] - 0s 50us/step - loss: 0.0987 - acc: 0.9676 - val_loss: 0.2826 - val_acc: 0.8633\n",
      "Epoch 14/17\n",
      "1700/1700 [==============================] - 0s 57us/step - loss: 0.0972 - acc: 0.9694 - val_loss: 0.2918 - val_acc: 0.8600\n",
      "Epoch 15/17\n",
      "1700/1700 [==============================] - 0s 54us/step - loss: 0.0959 - acc: 0.9676 - val_loss: 0.2960 - val_acc: 0.8633\n",
      "Epoch 16/17\n",
      "1700/1700 [==============================] - 0s 53us/step - loss: 0.0946 - acc: 0.9700 - val_loss: 0.2955 - val_acc: 0.8600\n",
      "Epoch 17/17\n",
      "1700/1700 [==============================] - 0s 47us/step - loss: 0.0935 - acc: 0.9682 - val_loss: 0.3008 - val_acc: 0.8567\n"
     ]
    }
   ],
   "source": [
    "#enter in keras model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=17, batch_size=None)#, batch_size=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182,  27],\n",
       "       [ 16,  75]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(y_test, axis=1), model.predict_classes(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
