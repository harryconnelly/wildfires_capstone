{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leverage logistic regression as this is classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (9,10,11,12,13,14,27,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#load in data\n",
    "df = pd.read_csv('lagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classification, traintestsplit, and counter\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "firesizecriteria=[]\n",
    "for i,y in enumerate(df['FIRE SIZE MAX']):\n",
    "    if i%100000==0:\n",
    "        print(i)\n",
    "    #if (float(y)-500) > float(df1000['diffmax'][i]):\n",
    "   \n",
    "\n",
    "    if df['diffmax'][i] > 20000:\n",
    "            firesizecriteria.append(1)\n",
    "    elif df['diffmax'][i]*5 < df['FIRE SIZE MAX'][i]:\n",
    "        if df['diffmax'][i] > 600:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    elif df['diffmax'][i]*2 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 1000:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "\n",
    "            \n",
    "    elif df['diffmax'][i]*4 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 500:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    elif df['diffmax'][i]*100 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 200:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    \n",
    "    else:\n",
    "        firesizecriteria.append(0)\n",
    "\n",
    "#     else:\n",
    "#         print(0)   \n",
    "#         firesizecriteria.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.get_dummies(df['day of week'], dummy_na=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df,df5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['firesize']=pd.DataFrame(firesizecriteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### X = df.drop(columns=['lat lon', 'day of week','day_of_month','level_0','index','A','diffmax','day','dates','B','C','D','E','F','G','longitude','FIRE SIZE MAX','firesize','fire binary','FIRE SIZE SUM', 'cause', 'sz_class', 'state', 'latitude', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-71fa45376bd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data = X\n",
    "scaler.fit(data)\n",
    "X2=scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2=['firesize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y = make_classification(n_classes=2, class_sep=2, weights=[.7,.3], n_samples=2000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomUnderSampler(random_state=None, ratio=0.4, replacement=False,\n",
       "          return_indices=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # doctest: +NORMALIZE_WHITESPACE\n",
    "rus = RandomUnderSampler(ratio = .4)\n",
    "rus.fit(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=.4, random_state = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    561\n",
       "1    239\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_test)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "#import various keras models and features\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform yvariables into categorical variables like dummy variables\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variables essential to plugging into the model\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "n_output = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add key features\n",
    "model.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model.add(Dense(n_output, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "1200/1200 [==============================] - 0s 307us/step - loss: 0.8769 - acc: 0.5617 - val_loss: 0.6275 - val_acc: 0.6700\n",
      "Epoch 2/10\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 0.5219 - acc: 0.7300 - val_loss: 0.3880 - val_acc: 0.8075\n",
      "Epoch 3/10\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 0.3353 - acc: 0.8483 - val_loss: 0.2551 - val_acc: 0.8988\n",
      "Epoch 4/10\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.2249 - acc: 0.9233 - val_loss: 0.1750 - val_acc: 0.9400\n",
      "Epoch 5/10\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 0.1591 - acc: 0.9592 - val_loss: 0.1240 - val_acc: 0.9725\n",
      "Epoch 6/10\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 0.1190 - acc: 0.9800 - val_loss: 0.0930 - val_acc: 0.9838\n",
      "Epoch 7/10\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.0951 - acc: 0.9875 - val_loss: 0.0728 - val_acc: 0.9962\n",
      "Epoch 8/10\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 0.0798 - acc: 0.9917 - val_loss: 0.0595 - val_acc: 0.9950\n",
      "Epoch 9/10\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 0.0697 - acc: 0.9917 - val_loss: 0.0504 - val_acc: 0.9950\n",
      "Epoch 10/10\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 0.0627 - acc: 0.9917 - val_loss: 0.0439 - val_acc: 0.9950\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=10, batch_size=None)#, batch_size=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[560,   1],\n",
       "       [  3, 236]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build a confusion matrix. left is jump, 1 is is no jump\n",
    "confusion_matrix(np.argmax(y_test, axis=1), model.predict_classes(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
