{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leverage logistic regression as this is classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (9,10,11,12,13,14,27,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#load in data\n",
    "df = pd.read_csv('lagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classification, traintestsplit, and counter\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dynamic Classifications Through Scoring System as to what is considred a large jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "firesizecriteria=[]\n",
    "for i,y in enumerate(df['FIRE SIZE MAX']):\n",
    "\n",
    "    if df['diffmax'][i] > 20000:\n",
    "            firesizecriteria.append(1)\n",
    "    elif df['diffmax'][i]*5 < df['FIRE SIZE MAX'][i]:\n",
    "        if df['diffmax'][i] > 600:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    elif df['diffmax'][i]*2 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 1000:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "\n",
    "            \n",
    "    elif df['diffmax'][i]*4 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 500:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    elif df['diffmax'][i]*100 < float(df['FIRE SIZE MAX'][i]):\n",
    "        if df['diffmax'][i] > 200:\n",
    "            firesizecriteria.append(1)\n",
    "        else:\n",
    "            firesizecriteria.append(0)\n",
    "    \n",
    "    else:\n",
    "        firesizecriteria.append(0)\n",
    "\n",
    "#     else:\n",
    "#         print(0)   \n",
    "#         firesizecriteria.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unncessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df, pd.DataFrame(firesizecriteria).rename(columns={0:'firesize_criteria'})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.get_dummies(df['day of week'], dummy_na=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df,df5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df55=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1',\n",
       "       'index', 'Unnamed: 0.1.1.1.1', 'dates', 'latit', 'longit', 'longitude',\n",
       "       'latitude', 'state', 'sz_class', 'cause', 'firesize', 'A', 'B', 'C',\n",
       "       'D', 'E', 'F', 'G', 'FIRE SIZE SUM', 'FIRE SIZE MAX', 'diffmax',\n",
       "       'fire binary', 'weekend', 'day', 'day of week', 'lat lon', '0', '0.1',\n",
       "       '0.2', '0.3', '0.4', '0.5', 'df_30_60_sum', 'df_30_60_max',\n",
       "       'df_30_B_60_sum', 'df_30_A_60_sum', 'df_30_C_60_sum', 'df_30_D_60_sum',\n",
       "       'df_30_E_60_sum', 'df_30_F_60_sum', 'df_30_G_60_sum', 'y4sum', 'y4max',\n",
       "       'y1sum', 'y1max', 'd45sum', 'd45max', 'df_30_60_sum.1',\n",
       "       'df_30_60_max.1', 'df_30_B_60_sum.1', 'df_30_A_60_sum.1',\n",
       "       'df_30_C_60_sum.1', 'df_30_D_60_sum.1', 'df_30_E_60_sum.1',\n",
       "       'df_30_F_60_sum.1', 'df_30_G_60_sum.1', 'firesize_criteria', 'Friday',\n",
       "       'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df55.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.sort_values('dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df55=df55.reset_index().drop(columns='level_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estabolish a y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df55['firesize_criteria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.drop(columns=['lat lon', 'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1', 'index', 'Unnamed: 0.1.1.1.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.drop(columns=['day', 'firesize_criteria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df55=df55.drop(columns=['day of week', 'cause', 'sz_class', 'state', 'longitude', 'latitude', 'firesize', 'df_30_A_60_sum.1', 'df_30_B_60_sum.1', 'df_30_C_60_sum.1', 'df_30_D_60_sum.1', 'df_30_E_60_sum.1', 'df_30_F_60_sum.1', 'df_30_G_60_sum.1', 'df_30_60_sum.1', 'df_30_60_max.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create X and Y variables split on Date. \n",
    "\n",
    "### This is to train based on dates. Test is assumed to be in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df55[:int(df55.shape[0]*.7)]\n",
    "X_test=df55[int(df55.shape[0]*.7):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardize The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y[:int(df55.shape[0]*.7)]\n",
    "y_test=y[int(df55.shape[0]*.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98119,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140171, 43)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df55.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make classifications of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_classification(n_classes=2, class_sep=2, weights=[.7,.3], n_samples=1700 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = make_classification(n_classes=2, class_sep=2, weights=[.7,.3], n_samples=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Keras to Use Neural Network that Classifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomUnderSampler(random_state=None, ratio=0.5, replacement=False,\n",
       "          return_indices=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # doctest: +NORMALIZE_WHITESPACE\n",
    "rus = RandomUnderSampler(ratio = .5)\n",
    "rus.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomUnderSampler(random_state=None, ratio=0.5, replacement=False,\n",
       "          return_indices=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # doctest: +NORMALIZE_WHITESPACE\n",
    "rus = RandomUnderSampler(ratio = .5)\n",
    "rus.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###put in input, outputs and hidden sizes\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "n_output = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add key features\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_hidden, input_dim=500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_hidden, input_dim=100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_hidden, input_dim=1000, activation='relu'))\n",
    "model.add(Dense(n_output, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#design model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1700 samples, validate on 300 samples\n",
      "Epoch 1/25\n",
      "1700/1700 [==============================] - 1s 334us/step - loss: 0.7174 - acc: 0.5397 - val_loss: 0.6119 - val_acc: 0.6783\n",
      "Epoch 2/25\n",
      "1700/1700 [==============================] - 0s 57us/step - loss: 0.5851 - acc: 0.6774 - val_loss: 0.5321 - val_acc: 0.7067\n",
      "Epoch 3/25\n",
      "1700/1700 [==============================] - 0s 54us/step - loss: 0.5158 - acc: 0.7159 - val_loss: 0.4876 - val_acc: 0.7233\n",
      "Epoch 4/25\n",
      "1700/1700 [==============================] - 0s 58us/step - loss: 0.4316 - acc: 0.7662 - val_loss: 0.4396 - val_acc: 0.7750\n",
      "Epoch 5/25\n",
      "1700/1700 [==============================] - 0s 51us/step - loss: 0.3668 - acc: 0.8147 - val_loss: 0.3954 - val_acc: 0.8300\n",
      "Epoch 6/25\n",
      "1700/1700 [==============================] - 0s 56us/step - loss: 0.3160 - acc: 0.8582 - val_loss: 0.3671 - val_acc: 0.8633\n",
      "Epoch 7/25\n",
      "1700/1700 [==============================] - 0s 60us/step - loss: 0.2731 - acc: 0.8879 - val_loss: 0.3546 - val_acc: 0.8467\n",
      "Epoch 8/25\n",
      "1700/1700 [==============================] - 0s 47us/step - loss: 0.2305 - acc: 0.9068 - val_loss: 0.3589 - val_acc: 0.8567\n",
      "Epoch 9/25\n",
      "1700/1700 [==============================] - 0s 60us/step - loss: 0.2151 - acc: 0.9156 - val_loss: 0.3432 - val_acc: 0.8450\n",
      "Epoch 10/25\n",
      "1700/1700 [==============================] - 0s 54us/step - loss: 0.1977 - acc: 0.9247 - val_loss: 0.3453 - val_acc: 0.8500\n",
      "Epoch 11/25\n",
      "1700/1700 [==============================] - 0s 50us/step - loss: 0.1845 - acc: 0.9350 - val_loss: 0.3553 - val_acc: 0.8467\n",
      "Epoch 12/25\n",
      "1700/1700 [==============================] - 0s 55us/step - loss: 0.1509 - acc: 0.9388 - val_loss: 0.3712 - val_acc: 0.8500\n",
      "Epoch 13/25\n",
      "1700/1700 [==============================] - 0s 55us/step - loss: 0.1826 - acc: 0.9374 - val_loss: 0.3605 - val_acc: 0.8567\n",
      "Epoch 14/25\n",
      "1700/1700 [==============================] - 0s 53us/step - loss: 0.1486 - acc: 0.9468 - val_loss: 0.3651 - val_acc: 0.8550\n",
      "Epoch 15/25\n",
      "1700/1700 [==============================] - 0s 55us/step - loss: 0.1590 - acc: 0.9479 - val_loss: 0.3552 - val_acc: 0.8600\n",
      "Epoch 16/25\n",
      "1700/1700 [==============================] - 0s 54us/step - loss: 0.1475 - acc: 0.9497 - val_loss: 0.3547 - val_acc: 0.8617\n",
      "Epoch 17/25\n",
      "1700/1700 [==============================] - 0s 48us/step - loss: 0.1499 - acc: 0.9471 - val_loss: 0.3593 - val_acc: 0.8583\n",
      "Epoch 18/25\n",
      "1700/1700 [==============================] - 0s 57us/step - loss: 0.1397 - acc: 0.9571 - val_loss: 0.3582 - val_acc: 0.8600\n",
      "Epoch 19/25\n",
      "1700/1700 [==============================] - 0s 54us/step - loss: 0.1299 - acc: 0.9529 - val_loss: 0.3632 - val_acc: 0.8583\n",
      "Epoch 20/25\n",
      "1700/1700 [==============================] - 0s 55us/step - loss: 0.1217 - acc: 0.9571 - val_loss: 0.3575 - val_acc: 0.8583\n",
      "Epoch 21/25\n",
      "1700/1700 [==============================] - 0s 55us/step - loss: 0.1293 - acc: 0.9529 - val_loss: 0.3645 - val_acc: 0.8550\n",
      "Epoch 22/25\n",
      "1700/1700 [==============================] - 0s 59us/step - loss: 0.1234 - acc: 0.9624 - val_loss: 0.3741 - val_acc: 0.8567\n",
      "Epoch 23/25\n",
      "1700/1700 [==============================] - 0s 55us/step - loss: 0.1222 - acc: 0.9591 - val_loss: 0.3759 - val_acc: 0.8500\n",
      "Epoch 24/25\n",
      "1700/1700 [==============================] - 0s 55us/step - loss: 0.1302 - acc: 0.9603 - val_loss: 0.3664 - val_acc: 0.8500\n",
      "Epoch 25/25\n",
      "1700/1700 [==============================] - 0s 58us/step - loss: 0.1289 - acc: 0.9615 - val_loss: 0.3756 - val_acc: 0.8467\n"
     ]
    }
   ],
   "source": [
    "#enter in keras model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=25, batch_size=None)#, batch_size=None, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[189,  19],\n",
       "       [ 27,  65]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.argmax(y_test, axis=1), model.predict_classes(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
